{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f8ea510-e1d0-4925-9457-d4415b442b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pyodbc in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: swifter in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: varname in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (3.4)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (2.36.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from swifter) (6.0.0)\n",
      "Requirement already satisfied: dask>=2.10.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2024.12.0)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from swifter) (4.66.5)\n",
      "Requirement already satisfied: executing<3.0,>=2.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from varname) (2.1.0)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2024.6.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.4.0)\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.20)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.6)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (17.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib_metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.20.2)\n",
      "Requirement already satisfied: locket in c:\\users\\yurii\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyodbc numpy scikit-image scikit-learn matplotlib Pillow swifter joblib opencv-python-headless varname scipy seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80548b7f-8d83-4186-80e7-45bc88415698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from varname import nameof\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.cluster import kmeans_plusplus\n",
    "from sklearn.cluster import k_means\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1126d43-2c6c-4e49-b63f-4b5e35575306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_otsu_threshold(image):\n",
    "    thresh, _ = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f25ff1-b8ac-47f3-a116-5db6a5f2769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr(image):\n",
    "    thresh = get_otsu_threshold(image)\n",
    "    ridge_pixels = image[image >= thresh]\n",
    "    valley_pixels = image[image < thresh]\n",
    "    mean_ridge = np.mean(ridge_pixels)\n",
    "    mean_valley = np.mean(valley_pixels)\n",
    "    var_ridge = np.var(ridge_pixels)\n",
    "    var_valley = np.var(valley_pixels)\n",
    "    snr_value = ((mean_ridge - mean_valley) ** 2 / (var_ridge + var_valley))\n",
    "    return snr_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe7e1c8-e5b8-477a-b68d-c39fcd84cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strength(image):\n",
    "    thresh = get_otsu_threshold(image)\n",
    "    ridge_pixels = image[image >= thresh]\n",
    "    valley_pixels = image[image < thresh]\n",
    "    mean_ridge = np.mean(ridge_pixels)\n",
    "    mean_valley = np.mean(valley_pixels)\n",
    "    return mean_ridge - mean_valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d868b0-f131-4d65-98a1-8aeb62b768a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuity(image):\n",
    "    thresh = get_otsu_threshold(image)\n",
    "    _, binary = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY_INV)\n",
    "    num, labels = cv2.connectedComponents(binary)\n",
    "    num_ridge_pixels = np.count_nonzero(binary)\n",
    "    return (num - 1) / num_ridge_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7cc321-2472-4136-8ee5-c73bfc5dcb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarity(image):\n",
    "    thresh = get_otsu_threshold(image)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    dilated = cv2.morphologyEx(image, cv2.MORPH_DILATE, kernel)\n",
    "    eroded = cv2.morphologyEx(image, cv2.MORPH_ERODE, kernel)\n",
    "\n",
    "    ridge_gradient = np.abs(image - eroded)\n",
    "    valley_gradient = np.abs(image - dilated)\n",
    "\n",
    "    _, ridge_mask = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)\n",
    "    _, valley_mask = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    ridge_clarity_map = np.multiply(ridge_mask, ridge_gradient) - np.multiply(valley_mask, valley_gradient)\n",
    "    clarity = np.sum(ridge_clarity_map) / len(image.ravel())\n",
    "\n",
    "    return clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c516f45f-3b7e-4125-af8b-b50603cf9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fp(df):\n",
    "    for i in range(1, 81):\n",
    "        for j in range(1, 81):\n",
    "            if (i-1) // 8 == (j-1) // 8:\n",
    "                continue\n",
    "            if float((df[f'{i}'][j]).strip(\"%\")) >= 50:\n",
    "                yield { i: j } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05bcfb62-97db-45aa-9d96-19103adbd8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fn(df):\n",
    "    for i in range(1, 81):\n",
    "        for j in range(1, 81):\n",
    "            if (i-1) // 8 != (j-1) // 8 or i == j:\n",
    "                continue\n",
    "            if float((df[f'{i}'][j]).strip(\"%\")) < 50:\n",
    "                yield { i: j } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe5fb45-4dc4-4d78-adda-f2fda73ab498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp(df):\n",
    "    for i in range(1, 81):\n",
    "        for j in range(1, 81):\n",
    "            if (i-1) // 8 != (j-1) // 8 or i == j:\n",
    "                continue\n",
    "            if float((df[f'{i}'][j]).strip(\"%\")) >= 50:\n",
    "                yield { i: j } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42201a9-83ef-4403-98a7-cdc9f0bc8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tn(df):\n",
    "    for i in range(1, 81):\n",
    "        for j in range(1, 81):\n",
    "            if (i-1) // 8 == (j-1) // 8:\n",
    "                continue\n",
    "            if float((df[f'{i}'][j]).strip(\"%\")) < 50:\n",
    "                yield { i: j } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7710411-d906-4450-9fcc-4ab9e16f64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_stats(df):\n",
    "    class_stats = {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0,\n",
    "        5: 0,\n",
    "        6: 0,\n",
    "        7: 0,\n",
    "        8: 0,\n",
    "        9: 0,\n",
    "        10: 0,\n",
    "    }\n",
    "    for i in range(0, 10):\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for j in range(i*8+1, (i+1)*8+1):\n",
    "            has_positive_inside = False\n",
    "            has_positive_outside = False\n",
    "            for k in range(1, 81):\n",
    "                score = float((df[f'{j}'][k]).strip(\"%\"))\n",
    "                if score >= 50:\n",
    "                    if (k-1) // 8 == i:\n",
    "                        has_positive_inside = True\n",
    "                    else:\n",
    "                        has_positive_outside = True\n",
    "            if has_positive_inside:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "            if has_positive_outside:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        class_stats[i+1] = {\n",
    "            \"TP\": tp,\n",
    "            \"FP\": fp,\n",
    "            \"TN\": tn,\n",
    "            \"FN\": fn\n",
    "        }\n",
    "    return class_stats\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06cbe813-a573-4aa8-99fa-642980021c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_stats_new(df, bound):\n",
    "    class_stats = {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0,\n",
    "        5: 0,\n",
    "        6: 0,\n",
    "        7: 0,\n",
    "        8: 0,\n",
    "        9: 0,\n",
    "        10: 0,\n",
    "    }\n",
    "    for i in range(0, 10):\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for j in range(1, 81):\n",
    "            score = df[f'group{i+1}'][j]\n",
    "            if score >= bound:\n",
    "                if (j-1) // 8 == i:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if (j-1) // 8 == i:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "        class_stats[i+1] = {\n",
    "            \"TP\": tp,\n",
    "            \"FP\": fp,\n",
    "            \"TN\": tn,\n",
    "            \"FN\": fn\n",
    "        }\n",
    "    return class_stats\n",
    "\n",
    "def get_macro_stats_new2(df, bound1, bound2, shift):\n",
    "    class_stats = {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0,\n",
    "        5: 0,\n",
    "        6: 0,\n",
    "        7: 0,\n",
    "        8: 0,\n",
    "        9: 0,\n",
    "        10: 0,\n",
    "    }\n",
    "    for i in range(0, 10):\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for j in range(shift+1, shift+81):\n",
    "            score1 = df[f'group{i+1}_pos'][j]\n",
    "            score2 = df[f'group{i+1}_mea'][j]\n",
    "            if score1 >= bound1 and score2 >= bound2:\n",
    "                if (j-shift-1) // 8 == i:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if (j-shift-1) // 8 == i:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "        class_stats[i+1] = {\n",
    "            \"TP\": tp,\n",
    "            \"FP\": fp,\n",
    "            \"TN\": tn,\n",
    "            \"FN\": fn\n",
    "        }\n",
    "    return class_stats\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da7dd92-0b73-4a70-a01a-e2eb155298e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_micro_stats(df, bound):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for j in range(1, 81):\n",
    "        has_positive_inside = False\n",
    "        has_positive_outside = False\n",
    "        for k in range(0, 80):\n",
    "            score = float((df[f'{j}'][k]).strip(\"%\"))\n",
    "            if score >= bound:\n",
    "                if k // 8 == (j-1) // 8:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if k // 8 == (j-1) // 8:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "        # if has_positive_inside:\n",
    "        #     tp += 1\n",
    "        # else:\n",
    "        #     fn += 1\n",
    "        # if has_positive_outside:\n",
    "        #     fp += 1\n",
    "        # else:\n",
    "        #     tn += 1\n",
    "    return {\n",
    "        \"TP\": tp,\n",
    "        \"FP\": fp,\n",
    "        \"TN\": tn,\n",
    "        \"FN\": fn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a93db47-59fc-4c6c-ae2c-e3f0fdf44a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_metrics(class_stats):\n",
    "    class_metrics = {}\n",
    "    for class_id in class_stats.keys():\n",
    "        tp = class_stats[class_id][\"TP\"]\n",
    "        fp = class_stats[class_id][\"FP\"]\n",
    "        tn = class_stats[class_id][\"TN\"]\n",
    "        fn = class_stats[class_id][\"FN\"]\n",
    "        acc = (tp+tn)/(tp+fp+tn+fn)*100\n",
    "        if tp+fp == 0:\n",
    "            pre = 100\n",
    "        else:\n",
    "            pre = tp/(tp+fp)*100\n",
    "        if tp+fn == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp/(tp+fn)*100\n",
    "        spec = tn/(tn+fp)*100\n",
    "        fpr = fp/(fp+tn)*100\n",
    "        if pre+rec == 0:\n",
    "            f1 = 0\n",
    "            f05 = 0\n",
    "        else:\n",
    "            f1 = 2*pre*rec/(pre+rec)\n",
    "            f05 = 1.25*pre*rec/(0.25*pre+rec)\n",
    "        class_metrics[class_id] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": pre,\n",
    "            \"recall\": rec,\n",
    "            \"specifity\": spec,\n",
    "            \"FPR\": fpr,\n",
    "            \"F1\": f1,\n",
    "            \"F0.5\": f05\n",
    "        }\n",
    "    return class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b8473fb-4b2f-44c7-89f2-dcdf39b4c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_average(macro_stats):\n",
    "    accuracies = np.array([value[\"accuracy\"] for _, value in macro_stats.items()])\n",
    "    precisions = np.array([value[\"precision\"] for _, value in macro_stats.items()])\n",
    "    recalls = np.array([value[\"recall\"] for _, value in macro_stats.items()])\n",
    "    specifities = np.array([value[\"specifity\"] for _, value in macro_stats.items()])\n",
    "    fprs = np.array([value[\"FPR\"] for _, value in macro_stats.items()])\n",
    "    f1s = np.array([value[\"F1\"] for _, value in macro_stats.items()]),\n",
    "    f05s = np.array([value[\"F0.5\"] for _, value in macro_stats.items()]),\n",
    "    return {\n",
    "        \"accuracy\": np.average(accuracies),\n",
    "        \"precision\": np.average(precisions),\n",
    "        \"recall\": np.average(recalls),\n",
    "        \"specifity\": np.average(specifities),\n",
    "        \"FPR\": np.average(fprs),\n",
    "        \"F1\": np.average(f1s),\n",
    "        \"F0.5\": np.average(f05s),\n",
    "        \"AUC ROC\": np.average(recalls) / 100 + np.average(specifities) / 100 - np.average(recalls) / 100 * np.average(specifities) / 100,\n",
    "        \"AUC PR\": np.average(recalls) / 100 * np.average(precisions) / 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b78215a-0a73-4667-8464-7f7255ada844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(metric_data, title):\n",
    "    for class_label, values in metric_data.items():\n",
    "        sns.histplot(data=values, bins=10, kde=True, label=class_label)\n",
    "    plt.legend(title=\"Classes\")\n",
    "    plt.title(f\"{title} histogram\")\n",
    "    plt.show()\n",
    "\n",
    "    data_to_plot = [values for values in metric_data.values()]\n",
    "    mins = [float(min(values)) for values in metric_data.values()]\n",
    "    maxs = [float(max(values)) for values in metric_data.values()]\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(data=data_to_plot, ax=ax)\n",
    "    plt.xticks(ticks=range(0, len(list(metric_data.keys()))), labels=list(metric_data.keys()))\n",
    "    plt.title(f\"{title} box plot\")\n",
    "    ax.grid(which='major', color='#DDDDDD', linewidth=0.8)\n",
    "    # Show the minor grid as well. Style it in very light gray as a thin,\n",
    "    # dotted line.\n",
    "    ax.grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "    # Make the minor ticks and gridlines show.\n",
    "    ax.minorticks_on()\n",
    "    ax.yaxis.grid(True) # Hide the horizontal gridlines\n",
    "    ax.xaxis.grid(False) # Show the vertical gridlines\n",
    "    plt.show()\n",
    "\n",
    "    sns.violinplot(data=data_to_plot, inner=\"quartile\")\n",
    "    plt.xticks(ticks=range(0, len(list(metric_data.keys()))), labels=list(metric_data.keys()))\n",
    "    plt.title(f\"{title} box plot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d9e04d-7a67-4a77-8cbf-b11ff107a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_micro_metrics(stats):\n",
    "    tp = stats[\"TP\"]\n",
    "    fp = stats[\"FP\"]\n",
    "    tn = stats[\"TN\"]\n",
    "    fn = stats[\"FN\"]\n",
    "    acc = (tp+tn)/(tp+fp+tn+fn)*100\n",
    "    pre = tp/(tp+fp)*100\n",
    "    rec = tp/(tp+fn)*100\n",
    "    spec = tn/(tn+fp)*100\n",
    "    fpr = fp/(fp+tn)*100\n",
    "    f1 = 2*pre*rec/(pre+rec)\n",
    "    f05 = 1.25*pre*rec/(0.25*pre+rec)\n",
    "    mcc = (tp*tn - fp*fn) / (math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    bac = (rec + spec) / 2\n",
    "    return {\n",
    "        # \"accuracy\": acc,\n",
    "        # \"precision\": pre,\n",
    "        # \"recall\": rec,\n",
    "        # \"specifity\": spec,\n",
    "        \"F0.5\": f05 / 100,\n",
    "        \"MCC\": mcc,\n",
    "        \"FPR\": fpr / 100,\n",
    "        # \"F1\": f1,\n",
    "        \"BAC\": bac / 100,\n",
    "        # \"AUC ROC\": (1 + rec - fpr) / 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a1cf5a7-a4b0-4559-a5c9-2b3301a62b80",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\PHD\\\\Fingerprint.Refactored\\\\Demo\\\\Demo\\\\bin\\\\Debug\\\\net9.0\\\\results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mPHD\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mFingerprint.Refactored\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDemo\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDemo\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mbin\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDebug\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mnet9.0\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mresults.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m stats \u001b[38;5;241m=\u001b[39m get_micro_stats(df, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m get_micro_metrics(stats)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\PHD\\\\Fingerprint.Refactored\\\\Demo\\\\Demo\\\\bin\\\\Debug\\\\net9.0\\\\results.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"E:\\\\PHD\\\\Fingerprint.Refactored\\\\Demo\\\\Demo\\\\bin\\\\Debug\\\\net9.0\\\\results.csv\")\n",
    "stats = get_micro_stats(df, 10)\n",
    "metrics = get_micro_metrics(stats)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c5e514-5416-4edc-b96f-602b9fff15ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"image.csv\")\n",
    "centroid, label, inertia = k_means(\n",
    "    df[['SNR', 'Ridge strength', 'Ridge continuity', 'Ridge clarity']].to_numpy(), n_clusters=4, n_init=\"auto\", random_state=0\n",
    ")\n",
    "pd.DataFrame(label, columns=['Label']).to_csv(\"test1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de89fe7a-5637-4b46-9cd7-29a45c0389f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 98.25, 'precision': 98.88888888888889, 'recall': 83.75, 'specifity': 99.86111111111111, 'FPR': 0.1388888888888889, 'F1': 88.74509803921569, 'F0.5': 93.81313131313131, 'AUC ROC': 0.9997743055555557, 'AUC PR': 0.8281944444444445}\n"
     ]
    }
   ],
   "source": [
    "# conn_string = ('Driver={ODBC Driver 18 for SQL Server};SERVER=localhost;'\n",
    "#                'Database=Fingerprint;TrustServerCertificate=yes;Trusted_Connection=yes;Integrated Security=yes;')\n",
    "# context = pyodbc.connect(conn_string)\n",
    "# image_info = pd.read_sql(\"SELECT * FROM Fingerprint.dbo.Images\", context)\n",
    "# snrs = {\n",
    "#     \"TP\": [],\n",
    "#     \"FP\": [],\n",
    "#     \"TN\": [],\n",
    "#     \"FN\": []\n",
    "# }\n",
    "# strengths = {\n",
    "#     \"TP\": [],\n",
    "#     \"FP\": [],\n",
    "#     \"TN\": [],\n",
    "#     \"FN\": []\n",
    "# }\n",
    "# continuities = {\n",
    "#     \"TP\": [],\n",
    "#     \"FP\": [],\n",
    "#     \"TN\": [],\n",
    "#     \"FN\": []\n",
    "# }\n",
    "# clarities = {\n",
    "#     \"TP\": [],\n",
    "#     \"FP\": [],\n",
    "#     \"TN\": [],\n",
    "#     \"FN\": []\n",
    "# }\n",
    "shift = 10\n",
    "df = pd.read_csv(\"results2_new2.csv\")\n",
    "df = df.set_index('Unnamed: 0')\n",
    "df1 = df.applymap(lambda x: float(x.strip(\"%\")))\n",
    "for i in range(0, 10):\n",
    "    j = (i+shift)*8\n",
    "    df_tmp = df1[[f'{j+1}', f'{j+2}', f'{j+3}', f'{j+4}', f'{j+5}', f'{j+6}', f'{j+7}', f'{j+8}']]\n",
    "    count = df_tmp[df_tmp >= 50.00].count(axis=1)\n",
    "    positives = df_tmp[df_tmp >= 50.00].mean(axis=1).fillna(0)\n",
    "    mean = df_tmp.mean(axis=1).fillna(0)\n",
    "    positives1 = pd.merge(positives.to_frame(), count.to_frame(), left_index=True, right_index=True)\n",
    "    mean1 = pd.merge(mean.to_frame(), count.to_frame(), left_index=True, right_index=True)\n",
    "    df_test_positive = pd.merge(df1, positives1, left_index=True, right_index=True)\n",
    "    df_test_mean = pd.merge(df1, mean1, left_index=True, right_index=True)\n",
    "    df1[f\"group{i+1}_pos\"] = df_test_positive['0_x']*df_test_positive['0_y']\n",
    "    df1[f\"group{i+1}_mea\"] = df_test_mean['0_x']*df_test_mean['0_y']\n",
    "df2 = df1[['group1_pos', 'group2_pos', 'group3_pos', 'group4_pos', 'group5_pos', 'group6_pos', 'group7_pos', 'group8_pos', 'group9_pos', 'group10_pos', 'group1_mea', 'group2_mea', 'group3_mea', 'group4_mea', 'group5_mea', 'group6_mea', 'group7_mea', 'group8_mea', 'group9_mea', 'group10_mea']] \n",
    "df2 = df2.applymap(lambda x: round(x, 2) / 8)\n",
    "# print(df2)\n",
    "stats = get_macro_stats_new2(df2, 15, 1, shift*8)\n",
    "metrics = get_macro_metrics(stats)\n",
    "avg = get_macro_average(metrics)\n",
    "print(avg)\n",
    "# results = {}\n",
    "# lists=[]\n",
    "# for b1 in range(1, 100):\n",
    "#     for b2 in range(1, 100):\n",
    "#         stats = get_macro_stats_new2(df2, b1, b2, shift*8)\n",
    "#         metrics = get_macro_metrics(stats)\n",
    "#         avg = get_macro_average(metrics)\n",
    "#         results[(b1, b2)] = avg\n",
    "#         # if (avg['FPR'] == 0.0):\n",
    "#         #     results[(b1, b2)] = avg\n",
    "#         #     continue\n",
    "# best_f1 = 0\n",
    "# best_key = None\n",
    "# best_value = None\n",
    "# for key, value in results.items():\n",
    "#     lists.append(value['F1'])\n",
    "#     if value['F1'] > best_f1:\n",
    "#         best_f1 = value['F1']\n",
    "#         best_key = key\n",
    "#         best_value = value\n",
    "# print(best_key)\n",
    "# print(best_value)\n",
    "# plt.plot(range(len(lists)), lists)\n",
    "# plt.show()\n",
    "# df2.to_csv(\"mean.csv\")\n",
    "# for i in range(0, 10):\n",
    "#     avg = (df1[f'{i*8+1}'] + df1[f'{i*8+2}'] + df1[f'{i*8+3}'] + df1[f'{i*8+4}'] + df1[f'{i*8+5}'] + df1[f'{i*8+6}'] + df1[f'{i*8+7}'] + df1[f'{i*8+8}']) / 8\n",
    "#     df1[f\"group{i+1}\"] = (abs(df1[f'{i*8+1}'] - avg) + abs(df1[f'{i*8+2}'] - avg) + abs(df1[f'{i*8+3}'] - avg) + abs(df1[f'{i*8+4}'] - avg) + abs(df1[f'{i*8+5}'] - avg) + abs(df1[f'{i*8+6}'] - avg) + abs(df1[f'{i*8+7}'] - avg) + abs(df1[f'{i*8+8}'] - avg)) / 8\n",
    "# df2 = df1[['group1', 'group2', 'group3', 'group4', 'group5', 'group6', 'group7', 'group8', 'group9', 'group10']] \n",
    "# df2 = df2.applymap(lambda x: round(x, 2))\n",
    "# df2.to_csv(\"mad.csv\")\n",
    "    # print(\"------------------------------------------METRICS (macro-approach)-------------------------------------------------------\")\n",
    "# macro_stats = get_macro_stats(df)\n",
    "# macro_metrics = get_macro_metrics(macro_stats)\n",
    "# for i in range(1, 11):\n",
    "#     print(f\"Class {i} metrics\")\n",
    "#     print(macro_metrics[i])\n",
    "# print(\"Averaged\")\n",
    "# print(get_macro_average(macro_metrics))\n",
    "# print(\"------------------------------------------METRICS (micro-approach)-------------------------------------------------------\")\n",
    "# micro_stats = get_micro_stats(df)\n",
    "# micro_metrics = get_micro_metrics(micro_stats)\n",
    "# print(micro_metrics)\n",
    "# for i in range(1, 81):\n",
    "#     for j in range(1, 81):\n",
    "#         if i == j:\n",
    "#             df[f\"{i}\"][j] = \"100.00%\"\n",
    "#         else:\n",
    "#             score = float((df[f'{i}'][j]).strip(\"%\"))\n",
    "#             row1 = image_info[image_info.Id == i].iloc[0]\n",
    "#             row2 = image_info[image_info.Id == j].iloc[0]\n",
    "#             image1 = cv2.imread(row1.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "#             image1 = image1[row1.HeightShift:row1.HeightOffset, row1.WidthShift:row1.WidthOffset]\n",
    "#             image2 = cv2.imread(row2.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "#             image2 = image2[row2.HeightShift:row2.HeightOffset, row2.WidthShift:row2.WidthOffset]\n",
    "#             snr_diff = abs(snr(image1) - snr(image2))\n",
    "#             strength_diff = abs(strength(image1) - strength(image2))\n",
    "#             cont_diff = abs(continuity(image1) - continuity(image2))\n",
    "#             clar_diff = abs(clarity(image1) - clarity(image2))\n",
    "#             if score >= 50:\n",
    "#                 if clar_diff > 80:\n",
    "#                     df[f\"{i}\"][j] = \"0.00%\"\n",
    "# print(len(list(get_fn(df))))\n",
    "# print(len(list(get_fp(df))))\n",
    "# print(len(list(get_tn(df))))\n",
    "# print(len(list(get_tp(df))))\n",
    "# # print(\"------------------------------FALSE NEGATIVES------------------------------------\")\n",
    "# # for pair in list(get_fn(df)):\n",
    "# #     for key in pair.keys():\n",
    "# #         row1 = image_info[image_info.Id == key].iloc[0]\n",
    "# #         row2 = image_info[image_info.Id == pair[key]].iloc[0]\n",
    "# #         image1 = cv2.imread(row1.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "# #         image1 = image1[row1.HeightShift:row1.HeightOffset, row1.WidthShift:row1.WidthOffset]\n",
    "# #         image2 = cv2.imread(row2.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "# #         image2 = image2[row2.HeightShift:row2.HeightOffset, row2.WidthShift:row2.WidthOffset]\n",
    "# #         snrs[\"FN\"].append(abs(snr(image1) - snr(image2)))\n",
    "# #         strengths[\"FN\"].append(abs(strength(image1) - strength(image2)))\n",
    "# #         continuities[\"FN\"].append(abs(continuity(image1) - continuity(image2)))\n",
    "# #         clarities[\"FN\"].append(abs(clarity(image1) - clarity(image2)))\n",
    "# # print(\"------------------------------FALSE POSITIVES------------------------------------\")\n",
    "# # for pair in list(get_fp(df)):\n",
    "# #     for key in pair.keys():\n",
    "# #         row1 = image_info[image_info.Id == key].iloc[0]\n",
    "# #         row2 = image_info[image_info.Id == pair[key]].iloc[0]\n",
    "# #         image1 = cv2.imread(row1.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "# #         image1 = image1[row1.HeightShift:row1.HeightOffset, row1.WidthShift:row1.WidthOffset]\n",
    "# #         image2 = cv2.imread(row2.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "# #         image2 = image2[row2.HeightShift:row2.HeightOffset, row2.WidthShift:row2.WidthOffset]\n",
    "# #         snrs[\"FP\"].append(abs(snr(image1) - snr(image2)))\n",
    "# #         strengths[\"FP\"].append(abs(strength(image1) - strength(image2)))\n",
    "# #         continuities[\"FP\"].append(abs(continuity(image1) - continuity(image2)))\n",
    "# #         clarities[\"FP\"].append(abs(clarity(image1) - clarity(image2)))\n",
    "# # print(\"------------------------------TRUE NEGATIVES------------------------------------\")\n",
    "# # for pair in list(get_tn(df)):\n",
    "# #     for key in pair.keys():\n",
    "# #         row1 = image_info[image_info.Id == key].iloc[0]\n",
    "# #         row2 = image_info[image_info.Id == pair[key]].iloc[0]\n",
    "# #         image1 = cv2.imread(row1.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "# #         image1 = image1[row1.HeightShift:row1.HeightOffset, row1.WidthShift:row1.WidthOffset]\n",
    "# #         image2 = cv2.imread(row2.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "# #         image2 = image2[row2.HeightShift:row2.HeightOffset, row2.WidthShift:row2.WidthOffset]\n",
    "# #         snrs[\"TN\"].append(abs(snr(image1) - snr(image2)))\n",
    "# #         strengths[\"TN\"].append(abs(strength(image1) - strength(image2)))\n",
    "# #         continuities[\"TN\"].append(abs(continuity(image1) - continuity(image2)))\n",
    "# #         clarities[\"TN\"].append(abs(clarity(image1) - clarity(image2)))\n",
    "# # print(\"------------------------------TRUE POSITIVES------------------------------------\")\n",
    "# test = pd.DataFrame(columns=['name', 'SNR', 'Ridge strength', 'Ridge continuity', 'Ridge clarity'])\n",
    "# for i in range(101, 111):\n",
    "#     for j in range(1, 9):\n",
    "#         image = cv2.imread(f'{i}_{j}.png', cv2.IMREAD_GRAYSCALE)\n",
    "#         test.loc[(i-101)*8+j] = [f'{i}_{j}.png', round(snr(image), 2), round(strength(image), 2), round(continuity(image)*1000, 2), round(clarity(image), 2)]\n",
    "# test.to_csv('image.csv')       \n",
    "# # for pair in list(get_tp(df)):\n",
    "# #     for key in pair.keys():\n",
    "# #         row1 = image_info[image_info.Id == key].iloc[0]\n",
    "# #         row2 = image_info[image_info.Id == pair[key]].iloc[0]\n",
    "# #         image1 = cv2.imread(row1.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "# #         image1 = image1[row1.HeightShift:row1.HeightOffset, row1.WidthShift:row1.WidthOffset]\n",
    "# #         image2 = cv2.imread(row2.FileName.replace(\".tif\", \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "# #         image2 = image2[row2.HeightShift:row2.HeightOffset, row2.WidthShift:row2.WidthOffset]\n",
    "# #         snrs[\"TP\"].append(abs(snr(image1) - snr(image2)))\n",
    "# #         strengths[\"TP\"].append(abs(strength(image1) - strength(image2)))\n",
    "# #         continuities[\"TP\"].append(abs(continuity(image1) - continuity(image2)))\n",
    "# #         clarities[\"TP\"].append(abs(clarity(image1) - clarity(image2)))\n",
    "# # snr_data = {\n",
    "# #     \"TP\": snrs[\"TP\"],\n",
    "# #     \"TN\": snrs[\"TN\"],\n",
    "# #     \"FP\": snrs[\"FP\"],\n",
    "# #     \"FN\": snrs[\"FN\"]\n",
    "# # }\n",
    "# # rs_data = {\n",
    "# #     \"TP\": strengths[\"TP\"],\n",
    "# #     \"TN\": strengths[\"TN\"],\n",
    "# #     \"FP\": strengths[\"FP\"],\n",
    "# #     \"FN\": strengths[\"FN\"]\n",
    "# # }\n",
    "# # rc_data = {\n",
    "# #     \"TP\": continuities[\"TP\"],\n",
    "# #     \"TN\": continuities[\"TN\"],\n",
    "# #     \"FP\": continuities[\"FP\"],\n",
    "# #     \"FN\": continuities[\"FN\"]\n",
    "# # }\n",
    "# # rcl_data = {\n",
    "# #     \"TP\": clarities[\"TP\"],\n",
    "# #     \"TN\": clarities[\"TN\"],\n",
    "# #     \"FP\": clarities[\"FP\"],\n",
    "# #     \"FN\": clarities[\"FN\"]\n",
    "# # }\n",
    "# # visualize(snr_data, \"SNR\")\n",
    "# # visualize(rs_data, \"Ridge strength\")\n",
    "# # visualize(rc_data, \"Ridge continuity\")\n",
    "# # visualize(rcl_data, \"Ridge clarity\")\n",
    "# # snr_data = {\n",
    "# #     \"Positive\": snrs[\"TP\"] + snrs[\"TN\"],\n",
    "# #     \"Negative\": snrs[\"FP\"] + snrs[\"FN\"]\n",
    "# # }\n",
    "# # rs_data = {\n",
    "# #     \"Positive\": strengths[\"TP\"] + strengths[\"TN\"],\n",
    "# #     \"Negative\": strengths[\"FP\"] + strengths[\"FN\"]\n",
    "# # }\n",
    "# # rc_data = {\n",
    "# #     \"Positive\": continuities[\"TP\"] + continuities[\"TN\"],\n",
    "# #     \"Negative\": continuities[\"FP\"] + continuities[\"FN\"]\n",
    "# # }\n",
    "# # rcl_data = {\n",
    "# #     \"Positive\": clarities[\"TP\"] + clarities[\"TN\"],\n",
    "# #     \"Negative\": clarities[\"FP\"] + clarities[\"FN\"]\n",
    "# # }\n",
    "# # visualize(snr_data, \"SNR\")\n",
    "# # visualize(rs_data, \"Ridge strength\")\n",
    "# # visualize(rc_data, \"Ridge continuity\")\n",
    "# # visualize(rcl_data, \"Ridge clarity\")\n",
    "# # snr_anova = f_oneway(snrs[\"TP\"] + snrs[\"TN\"], snrs[\"FP\"] + snrs[\"FN\"])\n",
    "# # strength_anova = f_oneway(strengths[\"TP\"] + strengths[\"TN\"], strengths[\"FP\"] + strengths[\"FN\"])\n",
    "# # continuity_anova = f_oneway(continuities[\"TP\"] + continuities[\"TN\"], continuities[\"FP\"] + continuities[\"FN\"])\n",
    "# # clarity_anova = f_oneway(clarities[\"TP\"] + clarities[\"TN\"], clarities[\"FP\"] + clarities[\"FN\"])\n",
    "# # print(snr_anova)\n",
    "# # print(strength_anova)\n",
    "# # print(continuity_anova)\n",
    "# # print(clarity_anova)\n",
    "# # for class_name in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
    "# #     print(class_name)\n",
    "# #     for metric in [snrs, strengths, continuities, clarities]:\n",
    "# #         print(f\"Mean = {np.mean(metric[class_name]):.2f}\")\n",
    "# #         print(f\"Median = {np.median(metric[class_name]):.2f}\")\n",
    "# #         print(f\"Standard deviation = {np.std(metric[class_name]):.2f}\")\n",
    "# #         print(f\"Variance = {np.var(metric[class_name]):.2f}\")\n",
    "# #         print(f\"25th, 50th, 75th percentile = {np.percentile(metric[class_name], [25, 50, 75])}\")\n",
    "# #         print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f2f23-0c3a-4121-8c39-b90c0b27ff5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
